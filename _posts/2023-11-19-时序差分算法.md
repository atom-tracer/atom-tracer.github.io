---
layout: post
title: 时序差分算法
categories: Learning Notes
tags: [Reinforce_Learning LAMDA]
---

## Introduction

动态规划算法要求马尔可夫决策过程是已知的，即要求与智能体交互的环境是完全已知的（例如迷宫或者给定规则的网格世界）。在此条件下，智能体其实并不需要和环境真正交互来采样数据，直接用动态规划算法就可以解出最优价值或策略。

但这在大部分场景下并不现实，机器学习的主要方法都是在数据分布未知的情况下针对具体的数据点来对模型做出更新的。对于**大部分强化学习现实场景（例如电子游戏或者一些复杂物理环境），其马尔可夫决策过程的状态转移概率是无法写出来的**，也就无法直接进行动态规划（当然，也不需要事先知道环境的奖励函数和状态转移函数）。在这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为**无模型的强化学习**（model-free reinforcement learning）。

本章介绍无模型的强化学习中的，基于**时序差分**（temporal difference，TD）的两大经典强化学习算法：

* Sarar
* Q-learning

还会引入在线策略学习和离线策略学习的概念：

* 在线策略学习要求使用在当前策略下采样得到的样本进行学习，一旦策略被更新，当前的样本就被放弃了，就好像在水龙头下用自来水洗手；
* 离线策略学习使用经验回放池将之前采样得到的样本收集起来再次利用，就好像使用脸盆接水后洗手；

因此，离线策略学习往往能够更好地利用历史数据，并具有更小的样本复杂度（算法达到收敛结果需要在环境中采样的样本数量），这使其被更广泛地应用。

## 时序差分方法

时序差分是一种用来**估计一个策略的价值函数**的方法，它结合了蒙特卡洛和动态规划算法的思想：

* 和蒙特卡洛的相似之处在于可以从样本数据中学习，不需要事先知道环境
* 和动态规划的相似之处在于根据贝尔曼方程的思想，利用后续状态的价值估计来更新当前状态的价值估计

时序差分方法的更新方式不再像蒙特卡洛方法那样严格地取期望。蒙特卡洛方法必须要等整个序列结束之后才能计算得到这一次的回报，而时序差分方法只需要当前步结束即可进行计算。

回顾一下蒙特卡洛方法对价值函数的增量更新方式：
$$
V(s_{t}) \larr V(s_{t})+\alpha[G_{t}-V(s_{t})]
$$
具体来说，时序差分算法用当前获得的奖励加上下一个状态的价值估计来作为在当前状态会获得的回报，即：
$$
V(s_{t}) \larr V(s_{t})+\alpha[r_{t}+\gamma V(s_{t+1})-V(s_{t})]
$$
其中$r_{t}+\gamma V(s_{t+1})-V(s_{t})$通常被称为**时序差分误差**，时序差分算法将其与步长$\alpha$的乘积作为状态价值的更新量。可以这么写的原因是：
$$
V_{\pi}(s)=\mathbb{E}_{\pi}[G_{t}|S_{t}=s] \\
= \mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k}|S_{t}=s] \\
= \mathbb{E}_{\pi}[R_{t}+\gamma \sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_{t}=s] \\
= \mathbb{E}_{\pi}[R_{t}+\gamma V_{\pi}(S_{t+1})|S_{t}=s]
$$
以下是TD学习的基本步骤：

1. 在状态s下选择一个动作a，并观察得到的奖励r和下一状态s'。
2. 计算时序差分误差：δ = r + γV(s') - V(s)。这里V(s)是当前状态s的价值函数估计，V(s')是下一状态s'的价值函数估计，γ是折扣因子。
3. 更新价值函数估计：V(s) = V(s) + αδ。这里α是学习率。

**也就是说，TD方法不需要进行完整的一条序列采样再更新，而是在一条序列中每一步都更新。**

## Sarsa 算法

策略评估已经可以通过时序差分算法实现，那么在不知道奖励函数和状态转移函数的情况下该怎么进行策略提升呢？答案是可以直接用时序差分算法来估计动作价值函数：
$$
Q(s_{t},a_{t}) \larr Q(s_{t},a_{t}) + \alpha[r_{t}+\gamma Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t})]
$$
然后我们用贪婪算法来选取在某个状态下动作价值最大的那个动作。

这个算法存在问题：

* 如果在策略提升中一直根据贪婪算法得到一个确定性策略，**可能会导致某些状态动作对永远没有在序列中出现**，以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。

简单常用的解决方案是不再一味使用贪婪算法，而是采用一个$\epsilon$-贪婪策略：有$1-\epsilon$的概率采用动作价值最大的那个动作，另外有$\epsilon$的概率从动作空间中随机采取一个动作。

Sarsa 的具体算法如下：

![image-20231119125421557](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231119125421557.png)