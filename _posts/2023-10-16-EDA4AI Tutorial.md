---
layout: post
title: EDA4AI Tutorial
categories: Research Notes
tags: [AI+EDA LAMDA]
---

## DNN Deployment Flow：A Naive Approach

DNN模型经过编译器进入硬件描述语言，这是指将深度神经网络模型转化为可以在特定硬件上执行的形式的过程。

首先，我们有一个DNN模型，这是一个用于执行特定任务（例如图像分类，语音识别等）的算法模型。这个模型通常是用高级编程语言（例如Python）编写的，并使用了特定的深度学习框架（例如TensorFlow或PyTorch）。

然后，我们使用一个编译器将这个模型转化为硬件描述语言（HDL）。硬件描述语言是一种专门用于描述数字逻辑电路和模拟电路的语言，例如VHDL或Verilog。编译器的工作就是将高级语言编写的程序转化为可以在特定硬件（例如GPU，FPGA，ASIC等）上运行的低级代码。

这个过程的目的是为了优化模型的执行效率。通过将模型转化为可以在特定硬件上运行的形式，我们可以充分利用硬件的特性，例如并行处理能力，以提高模型的运行速度和效率。这在处理大规模数据和复杂计算任务时尤其重要。

### HDL是什么？

硬件描述语言（Hardware Description Language，HDL）是一种用于描述数字电路和模拟电路的编程语言。它们被用于设计和模拟复杂的芯片和电路系统。最常见的硬件描述语言是 VHDL（VHSIC Hardware Description Language）和 Verilog。

这些语言看起来与我们常用的高级编程语言（如 Python 或 Java）有些不同。它们被设计用来描述硬件的行为和结构，因此它们包含了一些特殊的结构和语法。

例如，这是一个简单的 Verilog 代码片段，用于描述一个 2 输入的 AND 门：

```verilog
module AND_GATE (
    input wire A,
    input wire B,
    output wire Q
);
    assign Q = A & B;
endmodule
```

在这个例子中，`module` 是用来定义一个硬件模块的关键字，`input` 和 `output` 用来定义模块的输入和输出，`assign` 用来定义硬件的行为。

同样，这是一个简单的 VHDL 代码片段，用于描述同样的 2 输入 AND 门：

```vhdl
library ieee;
use ieee.std_logic_1164.all;

entity AND_GATE is
    port (
        A : in std_logic;
        B : in std_logic;
        Q : out std_logic
    );
end AND_GATE;

architecture behavior of AND_GATE is
begin
    Q <= A and B;
end behavior;
```

在这个例子中，`entity` 是用来定义一个硬件实体的关键字，`port` 用来定义实体的输入和输出，`architecture` 和 `begin` 用来定义硬件的行为。

这些代码片段都描述了同样的硬件行为：一个 AND 门，它接收两个输入 A 和 B，然后输出 A 和 B 的逻辑与结果。

### 不用HDL，而是编译

![image-20231016011544896](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016011544896.png)

![image-20231016012839555](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016012839555.png)

### MLIR（Multi-Level Intermediate Representation）

MLIR（Multi-Level Intermediate Representation）是一种用于优化机器学习模型的中间表示，它被设计用来支持任何模型的编译和转换。以下是MLIR能做到的一些事情：

1. **模型优化**：MLIR可以优化机器学习模型以提高其性能。这包括算法优化，如操作合并和常数折叠，以及硬件优化，如操作融合和数据布局转换。

2. **硬件抽象**：MLIR允许定义硬件的抽象表示，这使得它可以为特定的硬件目标优化模型。这包括CPU、GPU、TPU、FPGA等。

3. **编译流水线**：MLIR提供了一种用于构建和自定义编译流水线的框架。这允许开发者为特定的模型和硬件目标创建高效的编译流水线。

4. **模型转换**：MLIR可以将模型从一种表示转换为另一种表示。例如，它可以将TensorFlow模型转换为ONNX模型，或者将PyTorch模型转换为TensorFlow模型。

5. **跨框架兼容性**：MLIR可以帮助实现不同机器学习框架之间的兼容性。例如，它可以将TensorFlow模型转换为可以在PyTorch中运行的模型，反之亦然。

6. **代码生成**：MLIR可以为特定的硬件目标生成高效的代码。例如，它可以为GPU生成CUDA代码，或者为TPU生成TensorFlow代码。

总的来说，MLIR的目标是提供一种统一的、灵活的、可扩展的系统，以支持机器学习模型的全生命周期，从模型设计和训练，到模型优化和部署。

MLVM vs MLIR

![image-20231016011938323](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016011938323.png)

LLVM：操作底层，非常棘手

#### Torch-MLIR

![image-20231016011735159](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016011735159.png)

"Torch-MLIR" 是一个项目，其目标是为从 PyTorch 生态系统到 MLIR 生态系统提供一流的编译器支持。

该项目旨在创建一个高效、健壮的编译器，该编译器可以将 PyTorch（一个流行的深度学习框架）的模型和代码转换为 MLIR（多级中间表示）的形式。

**也就是说，这个编译器可以将某个DNN模型转换为中间表示，这个编译器又可以根据不同的目标平台进行代码的适配和优化。**

因此，Torch-MLIR 项目的目标就是建立一个桥梁，将 PyTorch 框架和 MLIR 编译器基础设施连接起来，从而使得 PyTorch 的模型和代码能够更好地优化和运行在各种不同的硬件平台上。

#### TPU-MLIR

这是一个针对Google的Tensor Processing Unit (TPU)的MLIR后端。它的目标是将MLIR表示的模型编译和优化以在TPU上运行。TPU是Google专门为机器学习应用设计的处理器，因此TPU-MLIR主要关注如何最大限度地利用TPU的特性和优势。

![image-20231016012449255](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016012449255.png)

##### 什么是Dialect？

在MLIR中，"Dialect"是一个非常重要的概念。它可以被看作是MLIR中间表示(IR)的一种子语言或子集，每种Dialect定义了一组特定的操作（Ops）和类型。这些操作和类型通常是针对特定的领域或硬件平台定义的。通过使用不同的Dialect，MLIR能够表示和优化各种各样的程序和算法。

1. Top Dialect：这可能是一个特定的Dialect，用于表示和优化特定的算法或程序。这个名字可能会因为具体的上下文而有所不同。在一些上下文中，"Top Dialect"可能指的是在编译和优化流程中最先使用的Dialect，也就是说，源代码首先被转换为这个Dialect。

2. TPU Dialect：这是一个针对Google TPU的Dialect。它定义了一组操作和类型，这些操作和类型是专门为TPU设计的，能够充分利用TPU的特性和优势。通过使用TPU Dialect，MLIR可以将程序优化和编译以在TPU上运行。

在MLIR（Multi-Level Intermediate Representation）框架中，Dialect可以被视作类似于硬件指令集架构（ISA）的概念。每个Dialect都定义了一组特定的操作和类型，这些操作和类型通常是针对特定的领域或硬件平台进行优化的。

例如，针对特定的硬件（如GPU、TPU等）或特定的计算任务（如线性代数、神经网络等），MLIR定义了一组相应的Dialect。这些Dialect可以帮助进行更有效的优化，以利用目标硬件的特性，提高程序运行的效率。

因此，可以将Dialect看作是MLIR框架中的“硬件指令集”，它们是MLIR优化和编译过程中的关键组成部分。

**这些Dialect就好比ISA的指令集，它们是针对目标硬件特性进行设计和优化的。**

**Torch-MLIR和TPU-MLIR的主要区别在于：Torch-MLIR关注如何将PyTorch模型转换为MLIR，而TPU-MLIR关注如何将MLIR模型优化和编译以在TPU上运行。**

### MLIR for Efficient Chip Design-CIRCT

![image-20231016013718781](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016013718781.png)

**CIRCT的本质是芯片设计工具，就和EDA一样。**

CIRCT（Circuit IR Compilers and Tools）项目是一个开源项目，其目标是使用MLIR（Multi-Level Intermediate Representation）框架来构建电路设计的编译器和相关工具。这个项目是为了解决硬件设计和验证中的一些常见问题，包括但不限于设计空间探索、硬件和软件的协同设计、以及硬件设计的验证和优化。

在MLIR框架的支持下，CIRCT项目可以利用MLIR的特性，例如模块化的设计、灵活的中间表示、以及强大的优化和转换能力，来实现高效的电路设计工具。

**芯片设计并不局限于一种特定的硬件平台。设计师可能需要在多种硬件平台上进行设计，这些硬件平台有着不同的特性和优势。**例如，ASIC通常用于高性能、高频率的应用，而FPGA则因其可编程性和灵活性而被广泛使用。

在这种背景下，CIRCT可以帮助设计师生成针对特定硬件平台优化的设计。通过使用CIRCT，设计师可以将他们的设计从高级硬件描述语言编译为针对特定硬件平台的低级表示或指令。这样，设计师可以根据目标硬件平台的特性和优势，实现最佳的性能和效率。

**当我们谈论"针对特定硬件"时，我们是指CIRCT能够生成针对特定硬件平台优化的设计。这些硬件平台可能包括ASIC（应用特定集成电路）、FPGA（现场可编程门阵列）或其他类型的硬件。**

**总的来说，CIRCT的目标是提供一种方法，可以在硬件设计的各个阶段进行优化，从而提高硬件的性能和效率。**

### 相关的其他工具

![image-20231016013939103](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016013939103.png)

![image-20231016013944835](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016013944835.png)

Calyx, Firrtl, Chisel，以及MLIR都是硬件设计和优化的重要工具，它们在设计流程中的角色和关系可以这样理解：

1. Chisel允许设计者用高级语言（Scala）编写硬件设计，然后将这些设计编译成Firrtl。
2. Firrtl是一个更接近硬件的中间表示，它可以进一步被优化和转换为低级硬件描述语言，如Verilog。
3. Calyx作为一种中间表示，专为编译器设计的硬件加速器，它的目标是提供一个框架，使得编译器开发者可以更容易地实现和优化新的硬件加速器。
4. MLIR则是一种更通用的中间表示，它可以表示各种不同的硬件和软件目标。MLIR可以用于优化机器学习模型，也可以与硬件设计工具链（如CIRCT）一起使用，以优化硬件设计。

在CIRCT项目中，MLIR被用作硬件设计的中间表示，用于连接高级硬件设计语言（如Chisel）和低级硬件实现。这意味着，设计者可以使用Chisel编写硬件设计，然后通过Firrtl和MLIR，将这些设计优化并转换为针对特定硬件平台的低级表示或指令。

## Arithmetic Unit Synthesis

算术单元合成（Arithmetic Unit Synthesis）是数字电路设计的一部分，专门处理算术运算单元的生成。这些单元可以执行各种基本的数学运算，如加法、减法、乘法、除法、**矩阵运算**等。

在算术单元合成过程中，设计者将使用硬件描述语言（如Verilog或VHDL）或高级硬件设计语言（如Chisel）来描述算术运算单元的行为或结构。然后，这些描述将被硬件合成工具转换为可以在FPGA或ASIC上实现的门级或寄存器传输级（RTL）设计。

**合成的结果可以根据所需的性能、功耗和面积进行优化。例如，针对高性能应用，设计者可能会选择实现并行的乘法器或加法器。对于面积和功耗敏感的应用，设计者可能会选择实现更节能、更小的算术单元。**

算术单元合成是现代数字电路设计中的关键步骤，因为它影响了系统的性能、功耗和成本。

![image-20231016015126581](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016015126581.png)

### 这个AUS怎么听起来没什么用？

你可能会问：加法、乘法这些算术单元不是早就有成熟的元件么？（事实上也是我的第一想法）

是的，加法器、减法器等基本算术单元已经有了成熟的设计和实现，它们在许多硬件设计库中都可以找到。然而：

* "算术单元合成"这个概念并不仅仅指的是这些基本运算单元的设计和实现。**在更广泛的意义上，它可以包括更复杂的、特定于应用的算术运算单元的设计和实现（例如矩阵运算）。**
* 不同的应用可能有不同的性能、功耗和面积需求。例如，高性能计算应用可能需要最快的算术运算单元，而嵌入式系统可能需要最节能或最小的算术运算单元。因此，根据特定应用的需求，可能需要设计和实现特定的算术运算单元。

可以理解为：我们希望有一个性能/功耗非常不错的**算子**来帮我们执行某一自定义的运算（或者基础运算）。

例子：在设计某一个组合逻辑电路中，如果我们不希望“竞争”“毛刺”（相关概念参考《数字逻辑与计算机组成》）等现象的出现，我们可能要减少电路的级数。（这或许会带来高功耗或者其他问题）

### Logic Synthesis vs Physical Synthesis

在集成电路设计中，Logic Synthesis和Physical Synthesis是两个重要的步骤。它们的定义如下：

* Logic Synthesis：逻辑综合是将设计人员的行为级描述（通常使用硬件描述语言，例如VHDL或Verilog）转换为网表的过程。这个网表包含了门级或者寄存器传输级别（RTL）的电路描述。逻辑综合的主要目标是在满足性能要求的同时，优化电路的面积、功耗和性能。**（例如，我可能希望将逻辑门的层数减少，比如我采用与非-与非表达式对应的逻辑电路（见《数字逻辑与计算机组成》））**

* Physical Synthesis：物理综合是在逻辑综合之后的步骤，它将逻辑综合生成的网表转换为物理布局。这个布局描述了电路的物理位置，包括门的位置、互连线的布线路径等。物理综合的目标是在满足设计规则和性能要求的同时，优化电路的面积、功耗和性能。**（关注实际效能）**

简单来说，逻辑综合主要关注的是功能和性能的实现，而物理综合则更关注电路的实际布局和布线，以满足制程规则和优化性能。

![image-20231016015709083](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016015709083.png)

"Constraints mapping between two synthesis stages is difficult"这句话是在讨论逻辑综合和物理综合之间约束条件的映射问题。

在集成电路设计流程中，设计者需要为逻辑综合和物理综合阶段设定一系列的约束条件，包括但不限于时序约束（例如设定最大延时）、面积约束、功耗约束等。这些约束条件被用来引导综合工具优化设计。

**然而，逻辑综合和物理综合阶段的约束条件是不同的，因为它们关注的设计层次和优化目标不同。**

将逻辑综合阶段的约束条件映射到物理综合阶段，或者从物理综合阶段反馈约束条件到逻辑综合阶段，都是一项具有挑战性的任务。

这是因为在这两个阶段之间，设计的表示和优化的目标都发生了变化，这导致了约束条件的直接映射变得困难。例如，逻辑综合阶段可能关注的是门级的优化，而物理综合阶段则需要考虑到实际的布局和布线效果：

* 在门级优化阶段，设计者可能会选择使用更少的逻辑门或更简洁的逻辑表达式来实现特定的功能，以减小电路的面积或降低功耗。
* 然而，这种优化可能会导致电路的布线复杂度增加，因为更简洁的逻辑可能会需要更复杂的布线来连接各个逻辑门。

因此，门级优化和物理布局优化需要同时考虑，以在性能、面积、功耗和可靠性之间找到一个平衡。在实际的集成电路设计中，这两种优化通常会交替进行，以逐步接近最优设计。

### Pareto Frontier（帕累托前沿）

Pareto Frontier（也被称为Pareto Front或Pareto Boundary）是一个在优化理论中使用的概念。

在多目标优化问题中，Pareto Frontier表示了在考虑所有目标时可能达到的最优解的集合。**在这些解中，没有一个解可以在不牺牲其他目标的情况下改进某一个目标。**换句话说，Pareto Frontier上的任何点都不能被任何其他可行解“主导”（即，其他解不能在所有目标上都优于它）。

例如，在集成电路设计中，设计师可能需要在性能、功耗和面积之间找到一个平衡。在这种情况下，Pareto Frontier将包括所有在这三个目标之间达到最佳平衡的设计。

#### Hyper-volume

![image-20231016020244916](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016020244916.png)

Hyper-volume，也被称为超体积或超体积指标，是一种在多目标优化中评估和比较Pareto前沿解集质量的度量方法。它的基本思想是计算Pareto前沿解集在目标空间中占据的区域或体积。

**在具体计算时，首先需要定义一个参考点，这个参考点通常是一个在所有目标上都比Pareto前沿解集差的点。（可自定）**然后，计算每一个Pareto前沿解到这个参考点在目标空间中形成的超立方体的体积，并将这些体积加起来，得到的总和就是Hyper-volume。

Hyper-volume的值越大，说明Pareto前沿解集的质量越高。因为一个更大的Hyper-volume意味着Pareto前沿解集在目标空间中占据了更大的区域，也就是说，这个解集提供了更多的选择，可以在不同目标之间做出更好的权衡。

Hyper-volume是一种非常有效的度量方法，因为它同时考虑了Pareto前沿解集的大小（即解的数量）和分布（即解在目标空间中的位置）。

**它用来衡量一个解集的质量。更进一步地，它可以用来衡量给出这些解集的模型。**

### A Learning Flow for DSE in Adder Synthesis

原文：[从架构综合到物理设计的学习桥梁，用于探索高能效高性能加法器 |IEEE会议出版物 |IEEE Xplore](https://ieeexplore.ieee.org/abstract/document/8009168)

这里以一个新手也能理解的语言大致介绍这篇论文的工作。

总结来说，这篇文章做了两件事情：

* **设计了构造更高效的前缀加法器的算法**
* **高效地对上述算法生成的解空间找到Pareto前沿**

#### 前缀加法器合成

**什么是前缀加法器？**

在二进制加法器设计中，最关键的部分是如何处理进位（Carry）的问题。传统的方法是串行处理，即从最低位开始，一位一位地进行加法和进位，这样的问题是速度慢。而并行前缀加法器的设计就是为了解决这个问题，它通过并行计算所有位的进位，大大提高了运算速度。

PGG算法就是实现并行前缀加法器的一种方法。它的基本思想是将加法器中的位分组，并在每个组内并行计算进位。具体步骤如下：

1. Propagate（传播）：对于每一位，如果这一位的加法不会影响更高位的进位，就标记为"传播"。
2. Generate（生成）：对于每一位，如果这一位的加法会产生一个进位，就标记为"生成"。
3. Group（分组）：将所有的位按照一定的规则分组，并在每个组内并行计算进位。

**这篇文章利用PGG算法的某个变种来生成一些前缀加法器的解决方案（集）。**

这篇文章的改进PGG算法基于某个工作，结合更多的修剪技术（例如前缀图结构的半正则性，非平凡扇入中的电平限制等）更好地探索加法器的设计空间。

这里的前缀加法器只存在于纸面上。也就是说，PGG算法虽然能给出前缀加法器的一些（比较好的）解，但是不能获知其性能开销，这件事必须送到EDA软件中进行仿真测试。

#### PGG算法存在的问题

##### 前缀节点大小

在加法器设计中，前缀节点是用于计算加法运算中的进位的一种结构，它是一种用于加速二进制加法的方法。**前缀节点大小通常指的是这种前缀结构的规模或复杂性。**

##### 逻辑和物理设计之间的差距

在PGG算法中，这篇工作专注于高性能加法器并探索逻辑电平$L=log_{2}n$的前缀加法器，因此这个架构阶段的指标是前缀节点大小s和最大扇出mfo（一个逻辑电路支持的最大的输出引脚数目）。**这两个指标是相互冲突的。即如果我们减少mfo，s就会增加，反之亦然。**

前缀节点大小自然不用说。减少最大扇出可以**缓解拥塞和负载分布，但是可能提高加法器的延迟。**所以我们需要在这两个指标中找到一个平衡点。**然而这两个指标并不呈完全的负相关关系：**

![image-20231018195901337](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018195901337.png)

可以粗略地理解为：前缀结构是理论，物理解决方案是实际。

我们需要知道：一种架构可以有不同的物理解决空间。在理论上（或者说平均上），A架构可能比B架构差，但是B架构的物理最优解可能比A架构的物理最优解要好。

然而，通过穷举所有架构的所有物理最优解是不可能的（使用EDA模拟某种架构的物理性能要花数个小时），且PGG算法生成了**数十万个**前缀图结构。为了解决两个设计阶段之间的**保真度差距**和高计算成本，这篇工作提出了一种新的机器学习引导设计空间探索，以取代穷举搜索。

#### 学习模型的准随机抽样

构建学习模型的逆境是如何选择训练数据，因为运行物理设计流程开销太大。同时，太少的训练数据可能会显着降低模型的准确性。一种方法是进行随机抽样来选择训练数据，但这可能不具有很好的代表性。（以上为论文原话）

为了解决这个问题，通过**观察**架构和物理解决方案空间的相关性，执行**架构驱动的准随机数据采样**。

在一次采样中，这个方法通过两级分箱（mfo和s）后进行随机选择，**这种方法旨在对覆盖不同架构箱的前缀加法器进行均匀采样。**分箱的主要级别由解决方案的mfo确定。但是，可能有数千个架构共享相同的mfo，因此分箱的二级基于s。之后，从这些辅助箱中随机选择加法器。

* 例子：给定 5000 个mfo = 4 的解决方案，我们想从中挑选 50 个解决方案。假设这 5000 个解决方案的大小分布从 244 到 258。首先从溶液桶中选取一个随机解决方案（mfo= 4，$s$ = 244）。然后我们从（mfo = 4，$s$ = 245）中随机选择一个解决方案，依此类推。从每个桶中选取 15 个溶液后，mfo= 4，我们再次从桶 （mfo = 4， $s$ = 244） 开始。重复此过程，直到我们得到 50 个解决方案。对其他 mfo values 也执行类似的过程。

#### 特征选择和学习模型

选择前缀加法器的节点大小和最大扇出（mfo）作为学习模型的两个主要特征。但是具有相同mfo的架构太多，所以还定义了总和路径扇出（spfo）参数

![image-20231018201304325](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018201304325.png)

（不需要太了解）

这篇工作的方法中，使用了spfo作为表征前缀结构的特征，还有mfo，s和critical delay等特征。**在不同的场景中，可能采用不同的特征。**

这篇文章探索了几种**监督学习技术**。例如SVR，LR，Lasso/Ridge，贝叶斯等。最终选择了带有RBF内核的SVR模型。

### 帕累托前沿驱动学习

#### 前言

探索Pareto边界的问题一般可以通过对某种前缀加法器架构的子集进行采样，并进行仿真来计算性能**（枚举）**。

然而，传统的机器学习问题旨在最小化预测准确性，而不是从解决方案集中探索帕累托边界。提高模型精度并不一定能改善帕累托边界，直接使用拟合模型进行帕累托边界探索甚至会错过多达60%的帕累托边界点。

这篇文章开发了一种算法方法，通过回归模型来探索Pareto模型。

考虑了帕累托前沿探索的两个空间：Delay&Area，Delay&Power。

对于任一空间，这两个指标之间存在很强的权衡。**对于延迟与功率空间的关系，我们建议使用联合输出功率延迟函数（PD）作为回归输出。**

![image-20231018202123569](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018202123569.png)

此外还有一个联合输出区域延迟（AD）函数，形式与含义类似：

![image-20231018202222130](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018202222130.png)

#### 原理（关键！）

那么，我们是如何利用这个模型来探索某个边界，而不是这个模型本身能做的，精准地回归预测呢？

##### alpha-sweep

这个方法基本上约等于超参数的探索。

通过更改权重alpha，模型倾向于最小化不同的指标（例如当alpha=0，模型就不关心Power了，而专注于减小Delay）。

所以这个模型的实际训练方法是：给定一个alpha的参数空间，对于每一个给定的超参数alpha，将其作为target进行训练。如果alpha的参数空间大小为10，那么这个方法会生成10个SVR模型。

在测试中，对于一组输入，扔进每个alpha值（对应的模型）进行预测。然后取每个alpha值中预测结果PD或者AD最低的10个架构值送到EDA中生成真实的数据点。这些“最优的点”就绘制成Pareto曲线。

真实点组成前沿和预测点的前沿：

![image-20231018203335366](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018203335366.png)

### Active Learning Flow

原文：[高速加法器的跨层优化：帕累托驱动的机器学习方法 |IEEE 期刊和杂志 |IEEE Xplore](https://ieeexplore.ieee.org/document/8509188)

这篇工作是A Learning Flow for DSE in Adder Synthesis的改进和延伸。主要做了PAL（Pareto Active Learning）工作。所以前后的部分我们跳过，直接来到关键环节——PAL环节。

#### 高斯回归预测

简称GP回归。GP模型给出的预测不再是单一的数值，而是均值和方差。其中均值代表了预测结果，方差代表结果的不确定性。

有关GP回归的介绍：[高斯过程回归：推导，实现和理解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/104601803)

了解即可。

#### PAL（Pareto Active Learning）

来看看GP回归给出的结果大概长什么样：

![image-20231018214534520](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018214534520.png)

（这个函数和这篇工作没有任何关系！）

另外需要注意的是：GP回归可以给出多维的结果。

##### 前置公式解释

给定输出的结果$(m,\sigma)$，定义一个hyper-rectangle HR：

![image-20231018220015583](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018220015583.png)

这里HR(x)就是对于一个样本，给出其预测结果（多维区间形式）。t表迭代次数。

**$R_{t}(x)$是是一个区间，而样本的值是PD或者AD结果！**

**下文中的“优超”指的是PD或者AD值的大于！**

在某一个时间步t， 对于某个样本的分类如下图所示。这里意思是：如果样本x优超于已选择集合中的所有点，则视其为Pareto前沿上的点。如果被优超，那么完全不可能是Pareto前沿上的点。此外就是不确定状态。

为什么不确定状态不直接丢掉呢？因为GP模型的更新可能缩小其不确定性，然后这个点可能就会优超于别的点了。

![image-20231018220414461](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018220414461.png)

在Pareto最优设计P和非分类设计U中选出不确定性最大的一个（只是以对角线长度衡量，还写这么复杂的形式，我服了…）：

![image-20231018220619778](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018220619778.png)

##### 算法过程

![image-20231018222426808](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018222426808.png)

![image-20231018222433501](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018222433501.png)

在GP回归模型中，仍然采用$\alpha-sweep$方法进行前沿的探索。

### Graph Learning for Adder Structure

原文：[高速加法器设计 通过图神经过程探索空间 |IEEE 期刊和杂志 |IEEE Xplore](https://ieeexplore.ieee.org/document/9542936)



#### 变分图自编码器

##### 自编码器

Auto-Encoder，中文称作自编码器，是一种**无监督式学习模型**。

自编码器主要包含两个部分：Encoder（编码器）和Decoder（解码器）。

* 编码器的作用是把高维输入X编码成低维的隐变量h，从而强迫神经网络学习最有信息量的特征；
* 解码器的作用是把隐藏层的隐变量h还原到初始维度，最好的状态就是解码器的输出能够完美地或者近似恢复出原来的输入，即$X^{R} =X$

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/v2-8f16ee4ca80f1d5057d1b5c656c94a61_720w.webp)

其中，Encoder和Decoder都是线性变换+激活函数。算法的优化目标就是最小化$X$和$X^{R}$的欧氏距离（MSE）。

**它有什么用处？**

自动编码器可以用于特征降维，类似主成分分析PCA，但是其相比PCA其性能更强，这是由于神经网络模型可以提取更有效的新特征。除了进行特征降维，自动编码器学习到的新特征可以送入有监督学习模型中，所以自动编码器可以起到特征提取器的作用。

例子：我有一张清晰图片，首先我通过编码器压缩这张图片的大小（如果展现出来可能比较模型），然后在需要解码的时候将其还原成清晰的图片。

有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作. 所以, 何不压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习. 这样学习起来就简单轻松了。

**需要注意什么？**

首先，在没有重建损失的情况下进行重要的降维通常会带来一个代价：隐空间（编码空间）中缺乏可解释和可利用的结构（**缺乏规则性，lack of regularity**）。其次，大多数时候，降维的最终目的不仅是减少数据的维数，而是要在减少维数的同时将数据主要的结构信息保留在简化的表示中。出于这两个原因，必须根据降维的最终目的来仔细控制和调整隐空间的大小和自编码器的“深度”（深度定义压缩的程度和质量）。

##### 变分自编码器（VAE）

可以参考文章：[半小时理解变分自编码器 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/144649293)

**自编码器用于内容生成的局限性**

此时，自然会想到一个问题：“自编码器和内容生成之间的联系是什么？”。确实，一旦对自编码器进行了训练，我们既有编码器又有解码器，但是仍然没有办法来产生任何新内容。乍一看，我们可能会认为，**如果隐空间足够规则（在训练过程中被编码器很好地“组织”了），我们可以从该隐空间中随机取一个点并将其解码以获得新的内容**，就像生成对抗网络中的生成器一样。

自编码器的高自由度使得可以在没有信息损失的情况下进行编码和解码（尽管隐空间的维数较低）但**会导致严重的过拟合**，这意味着隐空间的某些点将在解码时给出无意义的内容。

![image-20231029011527389](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231029011527389.png)

**变分自编码器的定义**

基于上述原因，我们希望能够保证隐空间的规律性。为此提出变分自编码器，**变分自编码器可以定义为一种自编码器，其训练经过正规化以避免过度拟合，并确保隐空间具有能够进行数据生成过程的良好属性。**

为了引入隐空间的某些正则化，我们对编码-解码过程进行了一些修改：**我们不是将输入编码为隐空间中的单个点，而是将其编码为隐空间中的概率分布**。然后对模型进行如下训练：

- 首先，将输入编码为在隐空间上的分布；
- 第二，从该分布中采样隐空间中的一个点；
- 第三，对采样点进行解码并计算出重建误差；
- 最后，重建误差通过网络反向传播。

将输入编码为具有一定方差而不是单个点的分布的原因是这样可以非常自然地表达隐空间规则化：编码器返回的分布被强制接近标准正态分布。

在训练VAE时最小化的损失函数由一个“重构项”（在最后一层）组成，“重构项”倾向于使编码解码方案尽可能地具有高性能，而一个“正则化项”（在隐层）通过使编码器返回的分布接近标准正态分布，来规范隐空间的组织。

![image-20231029012928081](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231029012928081.png)

>译者注：KL散度又称为相对熵，其定义为$KL(p(x), q(x))=\sum p(x)log\frac{p(x)}{q(x)}$。这个概念很重要，不只是VAE，很多地方都会用到。

**关于正则化的直观解释**

为了使生成过程成为可能，我们期望隐空间具有规则性，这可以通过两个主要属性表示：**连续性**（continuity，隐空间中的两个相邻点解码后不应呈现两个完全不同的内容）和**完整性**（completeness，针对给定的分布，从隐空间采样的点在解码后应提供“有意义”的内容）。

如果没有明确定义的正则化项，则模型可以学习最小化其重构误差，从而“忽略”要返回一个分布，最终表现得几乎像普通自编码器一样（导致过度拟合）。

具体地说，编码器可以返回具有微小方差的分布（往往是点分布，punctual distributions），或者返回具有巨大均值差异的分布（数据在隐空间中彼此相距很远）。在这两种情况下，返回分布的限制都没有取得效果，并且不满足连续性和/或完整性。

因此，为了避免这些影响，**我们必须同时对协方差矩阵和编码器返回的分布均值进行正则化**。实际上，通过强制分布接近标准正态分布（集中和简化）来完成此正则化。这样，我们要求协方差矩阵接近于单位阵，防止出现单点分布，并且均值接近于0，防止编码分布彼此相距太远。

![image-20231029013044711](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231029013044711.png)

使用此正则化项，我们可以防止模型在隐空间中的编码相互远离，并鼓励尽可能多的返回分布发生“重叠”，从而满足预期的连续性和完整性条件。

##### 图自编码器（GAE）

图卷积网络（GCN）可以被视为一种特征提取方法。在处理图结构数据时，GCN通过对节点及其邻居节点的信息进行聚合和转换，生成了新的、更高级的特征表示。

GCN就相当于一个以节点特征和邻接矩阵为输入、以节点嵌入信息为输出的函数，目的是为了得到节点的嵌入向量Z。

GAE是图卷积神经网络GCN在Auto-Encoders (AE)的应用，隐变量Z是图上的N个节点经过GCN后的N*F维特征，编码器就是两层GCN, 解码器就是向量点积。可以将隐变量Z理解为某种意义上图的节点的相似度，通过向量点积得到的两个图节点的相似度越大，则两个节点之间存在边的概率越大。

编码器是简单的两层图卷积网络：
$$
Z=G C N(X, A) 
$$
在这里，X是一个n*×*d的矩阵，其中n是图中节点的数量，d是每个节点的特征数量。A是一个n×n的矩阵，表示图的邻接关系，即节点之间是否存在边。Z是一个n×f的矩阵，其中f是嵌入空间的维度，这个矩阵包含了**每个节点的嵌入表示**。

对于一个两层的图卷积网络（GCN），映射函数会被执行两次。具体来说，每一层的图卷积操作可以被看作是一次映射函数的执行，它将输入的节点特征和图的结构信息（通常表示为邻接矩阵）映射到一个新的节点特征空间。

在第一层，映射函数会接收原始的节点特征和邻接矩阵作为输入，输出的是第一层的节点特征表示。然后，第二层的映射函数会接收第一层的输出作为输入，并再次使用邻接矩阵，产生最终的节点特征表示。这个过程可以看作是两次信息的“聚合”或“传播”，每一次都会考虑节点的邻居信息。

解码器同样是根据两点间存在边的概率来重构图(**节点嵌入表示相似，则两个点之间越有可能相连**）：
$$
\tilde{A}=\operatorname{sigmoid}\left(Z Z^{T}\right) 
$$
损失函数来衡量生成图和原始图间的差异：
$$
L=E_{q(Z | X, A)}[\log p(A | Z)]
$$

##### 变分图自编码器（VGAE）

参考：[VGAE：图变分自编码器 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/345337636)

**Model**

输入为邻接矩阵$A$和节点特征矩阵 $X$, 通过编码器（GCN）可以得到节点向量的低维表示高斯分布$(\mu, \sigma^2)$，然后通过解码器生成图结构（链路预测）。模型架构如下所示：

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/v2-3f8875f537e6aea94ecd2227ef08d84a_720w.webp)

**编码器**

在变分图自编码器（VGAE）中，我们使用变分自动编码器（VAE）代替普通的自动编码器。**在VAE中，编码器不直接输出节点嵌入，而是输出嵌入的均值和方差。**然后，我们可以从这个分布中采样得到节点嵌入。具体来说，编码器可以被表示为：
$$
q(Z | X, A)=\prod_{i=1}^{N} q\left(z_{i} | X, A\right)
$$
这是**节点嵌入的联合分布**。其中
$$
q\left(z_{i} | X, A\right)=N\left(z_{i} | \mu_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right)
$$
$\mu$表示节点向量的均值$\mu=G C N_{\mu}(X, A)$,$\sigma$表示节点向量的方差$\log \sigma=G C N_{\sigma}(X, A)$,这是由两个独立的GCN实现的。

两层图卷积网络的定义如下：
$$
G C N(X, A)=\tilde{A} \operatorname{ReLU}\left(\tilde{A} X W_{0}\right) W_{1}
$$
其中$\tilde{A}=D^{-1 / 2} A D^{-1 / 2}$，是对邻接矩阵进行归一化的结果，其中$D$是一个对角矩阵，其对角线元素是$A$的每一行的和（即每个节点的度）。两个GCN网络的形式都是如此，但是$\mu=G C N_{\mu}(X, A)$和$\log \sigma=G C N_{\sigma}(X, A)$共享$W_{0}$，$W_{1}$不同。采用变量这一步与变分自编码器相同都使用重参数技巧。

**解码器**

和VAE的相同。

#### 



#### Model：Diagram of the proposed graph neural process



在这个框架中，NP的输入是GAE的输出，即每个加法器结构的潜在表示。

##### 神经过程NP

神经过程（Neural Processes，简称NP）是一种新型的深度学习模型，它结合了深度神经网络和高斯过程的优点，以实现数据驱动的、不确定性的预测。

神经过程的主要思想是将数据集视为一个随机过程的观测结果。给定一组输入-输出对（例如，一组点的坐标和对应的函数值），神经过程可以学习这个随机过程，并对新的输入点进行预测。

神经过程的关键特性是它的模型容量可以随数据量的增加而增加。这使得神经过程在处理复杂和多变的数据时具有很大的优势。同时，神经过程还能够量化预测的不确定性，这是许多深度学习模型所不能做到的。

神经过程的一个重要应用是条件生成模型，例如条件图像生成或音频生成。在这些应用中，神经过程可以根据一组条件输入（例如，一组点的坐标）生成一组输出（例如，对应点的像素值或声音样本）。

具体地，NP采用了一个编码器-解码器结构，其中编码器将潜在表示映射到一个低维空间，解码器将低维表示映射回性能指标和不确定性的估计（是不是和VAE很像？）。

在训练过程中，NP使用最大似然估计来优化模型参数，以最大化训练数据的似然性。为了提高模型的泛化能力，NP还使用了一种称为dropout的正则化技术，以减少过拟合的风险。通过这种方式，NP可以学习到加法器结构和性能指标之间的复杂非线性关系，并提供对预测的不确定性的估计。

NP的输出是加法器的性能指标，例如面积、功耗和延迟，以及对这些指标的不确定性的估计。

**NP与GP**

神经过程（Neural Processes，简称NP）与高斯过程（Gaussian Processes，简称GP）有着密切的关系。事实上，神经过程可以被视为高斯过程的一种深度学习扩展。

高斯过程是一种强大的贝叶斯非参数方法，可以在给定数据的情况下对函数进行推断。它的主要优点是能够提供预测的不确定性度量，但是它的计算复杂性随数据量的增加而快速增加，对于大规模数据集来说，这可能是一个问题。

神经过程在某种程度上解决了这个问题。它结合了深度学习的优点，如能够从大规模数据中学习复杂模式的能力，以及高斯过程的优点，如能够提供预测的不确定性度量。神经过程的一个关键特性是它的模型容量可以随数据量的增加而增加，这使得神经过程在处理复杂和多变的数据时具有很大的优势。

总的来说，神经过程可以被视为一种尝试将深度学习和高斯过程的优点结合在一起的方法。

##### 基于顺序优化的DSE框架的工作流程

文中构造的GNP架构如下：

![image-20231029153311973](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231029153311973.png)

其中右下角的NP细节如下：

![图 5.- 神经过程的详细制度。](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/geng5-3114262-small.gif)

在这篇论文中，作者提出了一种基于顺序优化的DSE框架，用于高速加法器的设计空间探索。该框架的工作流程如下： 

* 初始化：首先，选择一组初始加法器结构，并使用EDA工具对其进行物理实现，以获得其性能指标。然后，使用这些数据来训练一个机器学习模型GNP，作为EDA工具的代理模型。 
* 顺序优化：在每次迭代中，利用不确定性信息来指导后续抽样。选择一个候选加法器结构，并使用代理模型GNP预测其性能指标。GNP的输出是加法器的性能指标，例如面积、功耗和延迟，以及对这些指标的不确定性的估计。
* 评估：在每次迭代中，使用EDA工具对所选加法器进行物理实现，并计算其实际性能指标。然后，将这些指标用于更新代理模型的训练数据，以提高其预测精度。在每次迭代中，使用GAE对当前最优加法器结构进行编码，以获得其潜在表示。然后，将这些表示用于训练NP，以预测加法器的性能指标和不确定性的估计。
* 终止条件：重复上述步骤，直到达到预定义的终止条件，例如达到最大迭代次数或找到满足设计目标和约束条件的最优加法器结构。

![image-20231029153237716](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231029153237716.png)EDA4AI Tutorial

## DNN Deployment Flow：A Naive Approach

DNN模型经过编译器进入硬件描述语言，这是指将深度神经网络模型转化为可以在特定硬件上执行的形式的过程。

首先，我们有一个DNN模型，这是一个用于执行特定任务（例如图像分类，语音识别等）的算法模型。这个模型通常是用高级编程语言（例如Python）编写的，并使用了特定的深度学习框架（例如TensorFlow或PyTorch）。

然后，我们使用一个编译器将这个模型转化为硬件描述语言（HDL）。硬件描述语言是一种专门用于描述数字逻辑电路和模拟电路的语言，例如VHDL或Verilog。编译器的工作就是将高级语言编写的程序转化为可以在特定硬件（例如GPU，FPGA，ASIC等）上运行的低级代码。

这个过程的目的是为了优化模型的执行效率。通过将模型转化为可以在特定硬件上运行的形式，我们可以充分利用硬件的特性，例如并行处理能力，以提高模型的运行速度和效率。这在处理大规模数据和复杂计算任务时尤其重要。

### HDL是什么？

硬件描述语言（Hardware Description Language，HDL）是一种用于描述数字电路和模拟电路的编程语言。它们被用于设计和模拟复杂的芯片和电路系统。最常见的硬件描述语言是 VHDL（VHSIC Hardware Description Language）和 Verilog。

这些语言看起来与我们常用的高级编程语言（如 Python 或 Java）有些不同。它们被设计用来描述硬件的行为和结构，因此它们包含了一些特殊的结构和语法。

例如，这是一个简单的 Verilog 代码片段，用于描述一个 2 输入的 AND 门：

```verilog
module AND_GATE (
    input wire A,
    input wire B,
    output wire Q
);
    assign Q = A & B;
endmodule
```

在这个例子中，`module` 是用来定义一个硬件模块的关键字，`input` 和 `output` 用来定义模块的输入和输出，`assign` 用来定义硬件的行为。

同样，这是一个简单的 VHDL 代码片段，用于描述同样的 2 输入 AND 门：

```vhdl
library ieee;
use ieee.std_logic_1164.all;

entity AND_GATE is
    port (
        A : in std_logic;
        B : in std_logic;
        Q : out std_logic
    );
end AND_GATE;

architecture behavior of AND_GATE is
begin
    Q <= A and B;
end behavior;
```

在这个例子中，`entity` 是用来定义一个硬件实体的关键字，`port` 用来定义实体的输入和输出，`architecture` 和 `begin` 用来定义硬件的行为。

这些代码片段都描述了同样的硬件行为：一个 AND 门，它接收两个输入 A 和 B，然后输出 A 和 B 的逻辑与结果。

### 不用HDL，而是编译

![image-20231016011544896](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016011544896.png)

![image-20231016012839555](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016012839555.png)

### MLIR（Multi-Level Intermediate Representation）

MLIR（Multi-Level Intermediate Representation）是一种用于优化机器学习模型的中间表示，它被设计用来支持任何模型的编译和转换。以下是MLIR能做到的一些事情：

1. **模型优化**：MLIR可以优化机器学习模型以提高其性能。这包括算法优化，如操作合并和常数折叠，以及硬件优化，如操作融合和数据布局转换。

2. **硬件抽象**：MLIR允许定义硬件的抽象表示，这使得它可以为特定的硬件目标优化模型。这包括CPU、GPU、TPU、FPGA等。

3. **编译流水线**：MLIR提供了一种用于构建和自定义编译流水线的框架。这允许开发者为特定的模型和硬件目标创建高效的编译流水线。

4. **模型转换**：MLIR可以将模型从一种表示转换为另一种表示。例如，它可以将TensorFlow模型转换为ONNX模型，或者将PyTorch模型转换为TensorFlow模型。

5. **跨框架兼容性**：MLIR可以帮助实现不同机器学习框架之间的兼容性。例如，它可以将TensorFlow模型转换为可以在PyTorch中运行的模型，反之亦然。

6. **代码生成**：MLIR可以为特定的硬件目标生成高效的代码。例如，它可以为GPU生成CUDA代码，或者为TPU生成TensorFlow代码。

总的来说，MLIR的目标是提供一种统一的、灵活的、可扩展的系统，以支持机器学习模型的全生命周期，从模型设计和训练，到模型优化和部署。

MLVM vs MLIR

![image-20231016011938323](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016011938323.png)

LLVM：操作底层，非常棘手

#### Torch-MLIR

![image-20231016011735159](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016011735159.png)

"Torch-MLIR" 是一个项目，其目标是为从 PyTorch 生态系统到 MLIR 生态系统提供一流的编译器支持。

该项目旨在创建一个高效、健壮的编译器，该编译器可以将 PyTorch（一个流行的深度学习框架）的模型和代码转换为 MLIR（多级中间表示）的形式。

**也就是说，这个编译器可以将某个DNN模型转换为中间表示，这个编译器又可以根据不同的目标平台进行代码的适配和优化。**

因此，Torch-MLIR 项目的目标就是建立一个桥梁，将 PyTorch 框架和 MLIR 编译器基础设施连接起来，从而使得 PyTorch 的模型和代码能够更好地优化和运行在各种不同的硬件平台上。

#### TPU-MLIR

这是一个针对Google的Tensor Processing Unit (TPU)的MLIR后端。它的目标是将MLIR表示的模型编译和优化以在TPU上运行。TPU是Google专门为机器学习应用设计的处理器，因此TPU-MLIR主要关注如何最大限度地利用TPU的特性和优势。

![image-20231016012449255](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016012449255.png)

##### 什么是Dialect？

在MLIR中，"Dialect"是一个非常重要的概念。它可以被看作是MLIR中间表示(IR)的一种子语言或子集，每种Dialect定义了一组特定的操作（Ops）和类型。这些操作和类型通常是针对特定的领域或硬件平台定义的。通过使用不同的Dialect，MLIR能够表示和优化各种各样的程序和算法。

1. Top Dialect：这可能是一个特定的Dialect，用于表示和优化特定的算法或程序。这个名字可能会因为具体的上下文而有所不同。在一些上下文中，"Top Dialect"可能指的是在编译和优化流程中最先使用的Dialect，也就是说，源代码首先被转换为这个Dialect。

2. TPU Dialect：这是一个针对Google TPU的Dialect。它定义了一组操作和类型，这些操作和类型是专门为TPU设计的，能够充分利用TPU的特性和优势。通过使用TPU Dialect，MLIR可以将程序优化和编译以在TPU上运行。

在MLIR（Multi-Level Intermediate Representation）框架中，Dialect可以被视作类似于硬件指令集架构（ISA）的概念。每个Dialect都定义了一组特定的操作和类型，这些操作和类型通常是针对特定的领域或硬件平台进行优化的。

例如，针对特定的硬件（如GPU、TPU等）或特定的计算任务（如线性代数、神经网络等），MLIR定义了一组相应的Dialect。这些Dialect可以帮助进行更有效的优化，以利用目标硬件的特性，提高程序运行的效率。

因此，可以将Dialect看作是MLIR框架中的“硬件指令集”，它们是MLIR优化和编译过程中的关键组成部分。

**这些Dialect就好比ISA的指令集，它们是针对目标硬件特性进行设计和优化的。**

**Torch-MLIR和TPU-MLIR的主要区别在于：Torch-MLIR关注如何将PyTorch模型转换为MLIR，而TPU-MLIR关注如何将MLIR模型优化和编译以在TPU上运行。**

### MLIR for Efficient Chip Design-CIRCT

![image-20231016013718781](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016013718781.png)

**CIRCT的本质是芯片设计工具，就和EDA一样。**

CIRCT（Circuit IR Compilers and Tools）项目是一个开源项目，其目标是使用MLIR（Multi-Level Intermediate Representation）框架来构建电路设计的编译器和相关工具。这个项目是为了解决硬件设计和验证中的一些常见问题，包括但不限于设计空间探索、硬件和软件的协同设计、以及硬件设计的验证和优化。

在MLIR框架的支持下，CIRCT项目可以利用MLIR的特性，例如模块化的设计、灵活的中间表示、以及强大的优化和转换能力，来实现高效的电路设计工具。

**芯片设计并不局限于一种特定的硬件平台。设计师可能需要在多种硬件平台上进行设计，这些硬件平台有着不同的特性和优势。**例如，ASIC通常用于高性能、高频率的应用，而FPGA则因其可编程性和灵活性而被广泛使用。

在这种背景下，CIRCT可以帮助设计师生成针对特定硬件平台优化的设计。通过使用CIRCT，设计师可以将他们的设计从高级硬件描述语言编译为针对特定硬件平台的低级表示或指令。这样，设计师可以根据目标硬件平台的特性和优势，实现最佳的性能和效率。

**当我们谈论"针对特定硬件"时，我们是指CIRCT能够生成针对特定硬件平台优化的设计。这些硬件平台可能包括ASIC（应用特定集成电路）、FPGA（现场可编程门阵列）或其他类型的硬件。**

**总的来说，CIRCT的目标是提供一种方法，可以在硬件设计的各个阶段进行优化，从而提高硬件的性能和效率。**

### 相关的其他工具

![image-20231016013939103](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016013939103.png)

![image-20231016013944835](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016013944835.png)

Calyx, Firrtl, Chisel，以及MLIR都是硬件设计和优化的重要工具，它们在设计流程中的角色和关系可以这样理解：

1. Chisel允许设计者用高级语言（Scala）编写硬件设计，然后将这些设计编译成Firrtl。
2. Firrtl是一个更接近硬件的中间表示，它可以进一步被优化和转换为低级硬件描述语言，如Verilog。
3. Calyx作为一种中间表示，专为编译器设计的硬件加速器，它的目标是提供一个框架，使得编译器开发者可以更容易地实现和优化新的硬件加速器。
4. MLIR则是一种更通用的中间表示，它可以表示各种不同的硬件和软件目标。MLIR可以用于优化机器学习模型，也可以与硬件设计工具链（如CIRCT）一起使用，以优化硬件设计。

在CIRCT项目中，MLIR被用作硬件设计的中间表示，用于连接高级硬件设计语言（如Chisel）和低级硬件实现。这意味着，设计者可以使用Chisel编写硬件设计，然后通过Firrtl和MLIR，将这些设计优化并转换为针对特定硬件平台的低级表示或指令。

## Arithmetic Unit Synthesis

算术单元合成（Arithmetic Unit Synthesis）是数字电路设计的一部分，专门处理算术运算单元的生成。这些单元可以执行各种基本的数学运算，如加法、减法、乘法、除法、**矩阵运算**等。

在算术单元合成过程中，设计者将使用硬件描述语言（如Verilog或VHDL）或高级硬件设计语言（如Chisel）来描述算术运算单元的行为或结构。然后，这些描述将被硬件合成工具转换为可以在FPGA或ASIC上实现的门级或寄存器传输级（RTL）设计。

**合成的结果可以根据所需的性能、功耗和面积进行优化。例如，针对高性能应用，设计者可能会选择实现并行的乘法器或加法器。对于面积和功耗敏感的应用，设计者可能会选择实现更节能、更小的算术单元。**

算术单元合成是现代数字电路设计中的关键步骤，因为它影响了系统的性能、功耗和成本。

![image-20231016015126581](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016015126581.png)

### 这个AUS怎么听起来没什么用？

你可能会问：加法、乘法这些算术单元不是早就有成熟的元件么？（事实上也是我的第一想法）

是的，加法器、减法器等基本算术单元已经有了成熟的设计和实现，它们在许多硬件设计库中都可以找到。然而：

* "算术单元合成"这个概念并不仅仅指的是这些基本运算单元的设计和实现。**在更广泛的意义上，它可以包括更复杂的、特定于应用的算术运算单元的设计和实现（例如矩阵运算）。**
* 不同的应用可能有不同的性能、功耗和面积需求。例如，高性能计算应用可能需要最快的算术运算单元，而嵌入式系统可能需要最节能或最小的算术运算单元。因此，根据特定应用的需求，可能需要设计和实现特定的算术运算单元。

可以理解为：我们希望有一个性能/功耗非常不错的**算子**来帮我们执行某一自定义的运算（或者基础运算）。

例子：在设计某一个组合逻辑电路中，如果我们不希望“竞争”“毛刺”（相关概念参考《数字逻辑与计算机组成》）等现象的出现，我们可能要减少电路的级数。（这或许会带来高功耗或者其他问题）

### Logic Synthesis vs Physical Synthesis

在集成电路设计中，Logic Synthesis和Physical Synthesis是两个重要的步骤。它们的定义如下：

* Logic Synthesis：逻辑综合是将设计人员的行为级描述（通常使用硬件描述语言，例如VHDL或Verilog）转换为网表的过程。这个网表包含了门级或者寄存器传输级别（RTL）的电路描述。逻辑综合的主要目标是在满足性能要求的同时，优化电路的面积、功耗和性能。**（例如，我可能希望将逻辑门的层数减少，比如我采用与非-与非表达式对应的逻辑电路（见《数字逻辑与计算机组成》））**

* Physical Synthesis：物理综合是在逻辑综合之后的步骤，它将逻辑综合生成的网表转换为物理布局。这个布局描述了电路的物理位置，包括门的位置、互连线的布线路径等。物理综合的目标是在满足设计规则和性能要求的同时，优化电路的面积、功耗和性能。**（关注实际效能）**

简单来说，逻辑综合主要关注的是功能和性能的实现，而物理综合则更关注电路的实际布局和布线，以满足制程规则和优化性能。

![image-20231016015709083](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016015709083.png)

"Constraints mapping between two synthesis stages is difficult"这句话是在讨论逻辑综合和物理综合之间约束条件的映射问题。

在集成电路设计流程中，设计者需要为逻辑综合和物理综合阶段设定一系列的约束条件，包括但不限于时序约束（例如设定最大延时）、面积约束、功耗约束等。这些约束条件被用来引导综合工具优化设计。

**然而，逻辑综合和物理综合阶段的约束条件是不同的，因为它们关注的设计层次和优化目标不同。**

将逻辑综合阶段的约束条件映射到物理综合阶段，或者从物理综合阶段反馈约束条件到逻辑综合阶段，都是一项具有挑战性的任务。

这是因为在这两个阶段之间，设计的表示和优化的目标都发生了变化，这导致了约束条件的直接映射变得困难。例如，逻辑综合阶段可能关注的是门级的优化，而物理综合阶段则需要考虑到实际的布局和布线效果：

* 在门级优化阶段，设计者可能会选择使用更少的逻辑门或更简洁的逻辑表达式来实现特定的功能，以减小电路的面积或降低功耗。
* 然而，这种优化可能会导致电路的布线复杂度增加，因为更简洁的逻辑可能会需要更复杂的布线来连接各个逻辑门。

因此，门级优化和物理布局优化需要同时考虑，以在性能、面积、功耗和可靠性之间找到一个平衡。在实际的集成电路设计中，这两种优化通常会交替进行，以逐步接近最优设计。

### Pareto Frontier（帕累托前沿）

Pareto Frontier（也被称为Pareto Front或Pareto Boundary）是一个在优化理论中使用的概念。

在多目标优化问题中，Pareto Frontier表示了在考虑所有目标时可能达到的最优解的集合。**在这些解中，没有一个解可以在不牺牲其他目标的情况下改进某一个目标。**换句话说，Pareto Frontier上的任何点都不能被任何其他可行解“主导”（即，其他解不能在所有目标上都优于它）。

例如，在集成电路设计中，设计师可能需要在性能、功耗和面积之间找到一个平衡。在这种情况下，Pareto Frontier将包括所有在这三个目标之间达到最佳平衡的设计。

#### Hyper-volume

![image-20231016020244916](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231016020244916.png)

Hyper-volume，也被称为超体积或超体积指标，是一种在多目标优化中评估和比较Pareto前沿解集质量的度量方法。它的基本思想是计算Pareto前沿解集在目标空间中占据的区域或体积。

**在具体计算时，首先需要定义一个参考点，这个参考点通常是一个在所有目标上都比Pareto前沿解集差的点。（可自定）**然后，计算每一个Pareto前沿解到这个参考点在目标空间中形成的超立方体的体积，并将这些体积加起来，得到的总和就是Hyper-volume。

Hyper-volume的值越大，说明Pareto前沿解集的质量越高。因为一个更大的Hyper-volume意味着Pareto前沿解集在目标空间中占据了更大的区域，也就是说，这个解集提供了更多的选择，可以在不同目标之间做出更好的权衡。

Hyper-volume是一种非常有效的度量方法，因为它同时考虑了Pareto前沿解集的大小（即解的数量）和分布（即解在目标空间中的位置）。

**它用来衡量一个解集的质量。更进一步地，它可以用来衡量给出这些解集的模型。**

### A Learning Flow for DSE in Adder Synthesis

原文：[从架构综合到物理设计的学习桥梁，用于探索高能效高性能加法器 |IEEE会议出版物 |IEEE Xplore](https://ieeexplore.ieee.org/abstract/document/8009168)

这里以一个新手也能理解的语言大致介绍这篇论文的工作。

总结来说，这篇文章做了两件事情：

* **设计了构造更高效的前缀加法器的算法**
* **高效地对上述算法生成的解空间找到Pareto前沿**

#### 前缀加法器合成

**什么是前缀加法器？**

在二进制加法器设计中，最关键的部分是如何处理进位（Carry）的问题。传统的方法是串行处理，即从最低位开始，一位一位地进行加法和进位，这样的问题是速度慢。而并行前缀加法器的设计就是为了解决这个问题，它通过并行计算所有位的进位，大大提高了运算速度。

PGG算法就是实现并行前缀加法器的一种方法。它的基本思想是将加法器中的位分组，并在每个组内并行计算进位。具体步骤如下：

1. Propagate（传播）：对于每一位，如果这一位的加法不会影响更高位的进位，就标记为"传播"。
2. Generate（生成）：对于每一位，如果这一位的加法会产生一个进位，就标记为"生成"。
3. Group（分组）：将所有的位按照一定的规则分组，并在每个组内并行计算进位。

**这篇文章利用PGG算法的某个变种来生成一些前缀加法器的解决方案（集）。**

这篇文章的改进PGG算法基于某个工作，结合更多的修剪技术（例如前缀图结构的半正则性，非平凡扇入中的电平限制等）更好地探索加法器的设计空间。

这里的前缀加法器只存在于纸面上。也就是说，PGG算法虽然能给出前缀加法器的一些（比较好的）解，但是不能获知其性能开销，这件事必须送到EDA软件中进行仿真测试。

#### PGG算法存在的问题

##### 前缀节点大小

在加法器设计中，前缀节点是用于计算加法运算中的进位的一种结构，它是一种用于加速二进制加法的方法。**前缀节点大小通常指的是这种前缀结构的规模或复杂性。**

##### 逻辑和物理设计之间的差距

在PGG算法中，这篇工作专注于高性能加法器并探索逻辑电平$L=log_{2}n$的前缀加法器，因此这个架构阶段的指标是前缀节点大小s和最大扇出mfo（一个逻辑电路支持的最大的输出引脚数目）。**这两个指标是相互冲突的。即如果我们减少mfo，s就会增加，反之亦然。**

前缀节点大小自然不用说。减少最大扇出可以**缓解拥塞和负载分布，但是可能提高加法器的延迟。**所以我们需要在这两个指标中找到一个平衡点。**然而这两个指标并不呈完全的负相关关系：**

![image-20231018195901337](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018195901337.png)

可以粗略地理解为：前缀结构是理论，物理解决方案是实际。

我们需要知道：一种架构可以有不同的物理解决空间。在理论上（或者说平均上），A架构可能比B架构差，但是B架构的物理最优解可能比A架构的物理最优解要好。

然而，通过穷举所有架构的所有物理最优解是不可能的（使用EDA模拟某种架构的物理性能要花数个小时），且PGG算法生成了**数十万个**前缀图结构。为了解决两个设计阶段之间的**保真度差距**和高计算成本，这篇工作提出了一种新的机器学习引导设计空间探索，以取代穷举搜索。

#### 学习模型的准随机抽样

构建学习模型的逆境是如何选择训练数据，因为运行物理设计流程开销太大。同时，太少的训练数据可能会显着降低模型的准确性。一种方法是进行随机抽样来选择训练数据，但这可能不具有很好的代表性。（以上为论文原话）

为了解决这个问题，通过**观察**架构和物理解决方案空间的相关性，执行**架构驱动的准随机数据采样**。

在一次采样中，这个方法通过两级分箱（mfo和s）后进行随机选择，**这种方法旨在对覆盖不同架构箱的前缀加法器进行均匀采样。**分箱的主要级别由解决方案的mfo确定。但是，可能有数千个架构共享相同的mfo，因此分箱的二级基于s。之后，从这些辅助箱中随机选择加法器。

* 例子：给定 5000 个mfo = 4 的解决方案，我们想从中挑选 50 个解决方案。假设这 5000 个解决方案的大小分布从 244 到 258。首先从溶液桶中选取一个随机解决方案（mfo= 4，$s$ = 244）。然后我们从（mfo = 4，$s$ = 245）中随机选择一个解决方案，依此类推。从每个桶中选取 15 个溶液后，mfo= 4，我们再次从桶 （mfo = 4， $s$ = 244） 开始。重复此过程，直到我们得到 50 个解决方案。对其他 mfo values 也执行类似的过程。

#### 特征选择和学习模型

选择前缀加法器的节点大小和最大扇出（mfo）作为学习模型的两个主要特征。但是具有相同mfo的架构太多，所以还定义了总和路径扇出（spfo）参数

![image-20231018201304325](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018201304325.png)

（不需要太了解）

这篇工作的方法中，使用了spfo作为表征前缀结构的特征，还有mfo，s和critical delay等特征。**在不同的场景中，可能采用不同的特征。**

这篇文章探索了几种**监督学习技术**。例如SVR，LR，Lasso/Ridge，贝叶斯等。最终选择了带有RBF内核的SVR模型。

### 帕累托前沿驱动学习

#### 前言

探索Pareto边界的问题一般可以通过对某种前缀加法器架构的子集进行采样，并进行仿真来计算性能**（枚举）**。

然而，传统的机器学习问题旨在最小化预测准确性，而不是从解决方案集中探索帕累托边界。提高模型精度并不一定能改善帕累托边界，直接使用拟合模型进行帕累托边界探索甚至会错过多达60%的帕累托边界点。

这篇文章开发了一种算法方法，通过回归模型来探索Pareto模型。

考虑了帕累托前沿探索的两个空间：Delay&Area，Delay&Power。

对于任一空间，这两个指标之间存在很强的权衡。**对于延迟与功率空间的关系，我们建议使用联合输出功率延迟函数（PD）作为回归输出。**

![image-20231018202123569](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018202123569.png)

此外还有一个联合输出区域延迟（AD）函数，形式与含义类似：

![image-20231018202222130](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018202222130.png)

#### 原理（关键！）

那么，我们是如何利用这个模型来探索某个边界，而不是这个模型本身能做的，精准地回归预测呢？

##### alpha-sweep

这个方法基本上约等于超参数的探索。

通过更改权重alpha，模型倾向于最小化不同的指标（例如当alpha=0，模型就不关心Power了，而专注于减小Delay）。

所以这个模型的实际训练方法是：给定一个alpha的参数空间，对于每一个给定的超参数alpha，将其作为target进行训练。如果alpha的参数空间大小为10，那么这个方法会生成10个SVR模型。

在测试中，对于一组输入，扔进每个alpha值（对应的模型）进行预测。然后取每个alpha值中预测结果PD或者AD最低的10个架构值送到EDA中生成真实的数据点。这些“最优的点”就绘制成Pareto曲线。

真实点组成前沿和预测点的前沿：

![image-20231018203335366](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018203335366.png)

### Active Learning Flow

原文：[高速加法器的跨层优化：帕累托驱动的机器学习方法 |IEEE 期刊和杂志 |IEEE Xplore](https://ieeexplore.ieee.org/document/8509188)

这篇工作是A Learning Flow for DSE in Adder Synthesis的改进和延伸。主要做了PAL（Pareto Active Learning）工作。所以前后的部分我们跳过，直接来到关键环节——PAL环节。

#### 高斯回归预测

简称GP回归。GP模型给出的预测不再是单一的数值，而是均值和方差。其中均值代表了预测结果，方差代表结果的不确定性。

有关GP回归的介绍：[高斯过程回归：推导，实现和理解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/104601803)

了解即可。

#### PAL（Pareto Active Learning）

来看看GP回归给出的结果大概长什么样：

![image-20231018214534520](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018214534520.png)

（这个函数和这篇工作没有任何关系！）

另外需要注意的是：GP回归可以给出多维的结果。

##### 前置公式解释

给定输出的结果$(m,\sigma)$，定义一个hyper-rectangle HR：

![image-20231018220015583](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018220015583.png)

这里HR(x)就是对于一个样本，给出其预测结果（多维区间形式）。t表迭代次数。

**$R_{t}(x)$是是一个区间，而样本的值是PD或者AD结果！**

**下文中的“优超”指的是PD或者AD值的大于！**

在某一个时间步t， 对于某个样本的分类如下图所示。这里意思是：如果样本x优超于已选择集合中的所有点，则视其为Pareto前沿上的点。如果被优超，那么完全不可能是Pareto前沿上的点。此外就是不确定状态。

为什么不确定状态不直接丢掉呢？因为GP模型的更新可能缩小其不确定性，然后这个点可能就会优超于别的点了。

![image-20231018220414461](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018220414461.png)

在Pareto最优设计P和非分类设计U中选出不确定性最大的一个（只是以对角线长度衡量，还写这么复杂的形式，我服了…）：

![image-20231018220619778](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018220619778.png)

##### 算法过程

![image-20231018222426808](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018222426808.png)

![image-20231018222433501](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231018222433501.png)

在GP回归模型中，仍然采用$\alpha-sweep$方法进行前沿的探索。

### Graph Learning for Adder Structure

原文：[高速加法器设计 通过图神经过程探索空间 |IEEE 期刊和杂志 |IEEE Xplore](https://ieeexplore.ieee.org/document/9542936)



#### 变分图自编码器

##### 自编码器

Auto-Encoder，中文称作自编码器，是一种**无监督式学习模型**。

自编码器主要包含两个部分：Encoder（编码器）和Decoder（解码器）。

* 编码器的作用是把高维输入X编码成低维的隐变量h，从而强迫神经网络学习最有信息量的特征；
* 解码器的作用是把隐藏层的隐变量h还原到初始维度，最好的状态就是解码器的输出能够完美地或者近似恢复出原来的输入，即$X^{R} =X$

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/v2-8f16ee4ca80f1d5057d1b5c656c94a61_720w.webp)

其中，Encoder和Decoder都是线性变换+激活函数。算法的优化目标就是最小化$X$和$X^{R}$的欧氏距离（MSE）。

**它有什么用处？**

自动编码器可以用于特征降维，类似主成分分析PCA，但是其相比PCA其性能更强，这是由于神经网络模型可以提取更有效的新特征。除了进行特征降维，自动编码器学习到的新特征可以送入有监督学习模型中，所以自动编码器可以起到特征提取器的作用。

例子：我有一张清晰图片，首先我通过编码器压缩这张图片的大小（如果展现出来可能比较模型），然后在需要解码的时候将其还原成清晰的图片。

有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作. 所以, 何不压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习. 这样学习起来就简单轻松了。

**需要注意什么？**

首先，在没有重建损失的情况下进行重要的降维通常会带来一个代价：隐空间（编码空间）中缺乏可解释和可利用的结构（**缺乏规则性，lack of regularity**）。其次，大多数时候，降维的最终目的不仅是减少数据的维数，而是要在减少维数的同时将数据主要的结构信息保留在简化的表示中。出于这两个原因，必须根据降维的最终目的来仔细控制和调整隐空间的大小和自编码器的“深度”（深度定义压缩的程度和质量）。

##### 变分自编码器（VAE）

可以参考文章：[半小时理解变分自编码器 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/144649293)

**自编码器用于内容生成的局限性**

此时，自然会想到一个问题：“自编码器和内容生成之间的联系是什么？”。确实，一旦对自编码器进行了训练，我们既有编码器又有解码器，但是仍然没有办法来产生任何新内容。乍一看，我们可能会认为，**如果隐空间足够规则（在训练过程中被编码器很好地“组织”了），我们可以从该隐空间中随机取一个点并将其解码以获得新的内容**，就像生成对抗网络中的生成器一样。

自编码器的高自由度使得可以在没有信息损失的情况下进行编码和解码（尽管隐空间的维数较低）但**会导致严重的过拟合**，这意味着隐空间的某些点将在解码时给出无意义的内容。

![image-20231029011527389](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231029011527389.png)

**变分自编码器的定义**

基于上述原因，我们希望能够保证隐空间的规律性。为此提出变分自编码器，**变分自编码器可以定义为一种自编码器，其训练经过正规化以避免过度拟合，并确保隐空间具有能够进行数据生成过程的良好属性。**

为了引入隐空间的某些正则化，我们对编码-解码过程进行了一些修改：**我们不是将输入编码为隐空间中的单个点，而是将其编码为隐空间中的概率分布**。然后对模型进行如下训练：

- 首先，将输入编码为在隐空间上的分布；
- 第二，从该分布中采样隐空间中的一个点；
- 第三，对采样点进行解码并计算出重建误差；
- 最后，重建误差通过网络反向传播。

将输入编码为具有一定方差而不是单个点的分布的原因是这样可以非常自然地表达隐空间规则化：编码器返回的分布被强制接近标准正态分布。

在训练VAE时最小化的损失函数由一个“重构项”（在最后一层）组成，“重构项”倾向于使编码解码方案尽可能地具有高性能，而一个“正则化项”（在隐层）通过使编码器返回的分布接近标准正态分布，来规范隐空间的组织。

![image-20231029012928081](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231029012928081.png)

>译者注：KL散度又称为相对熵，其定义为$KL(p(x), q(x))=\sum p(x)log\frac{p(x)}{q(x)}$。这个概念很重要，不只是VAE，很多地方都会用到。

**关于正则化的直观解释**

为了使生成过程成为可能，我们期望隐空间具有规则性，这可以通过两个主要属性表示：**连续性**（continuity，隐空间中的两个相邻点解码后不应呈现两个完全不同的内容）和**完整性**（completeness，针对给定的分布，从隐空间采样的点在解码后应提供“有意义”的内容）。

如果没有明确定义的正则化项，则模型可以学习最小化其重构误差，从而“忽略”要返回一个分布，最终表现得几乎像普通自编码器一样（导致过度拟合）。

具体地说，编码器可以返回具有微小方差的分布（往往是点分布，punctual distributions），或者返回具有巨大均值差异的分布（数据在隐空间中彼此相距很远）。在这两种情况下，返回分布的限制都没有取得效果，并且不满足连续性和/或完整性。

因此，为了避免这些影响，**我们必须同时对协方差矩阵和编码器返回的分布均值进行正则化**。实际上，通过强制分布接近标准正态分布（集中和简化）来完成此正则化。这样，我们要求协方差矩阵接近于单位阵，防止出现单点分布，并且均值接近于0，防止编码分布彼此相距太远。

![image-20231029013044711](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231029013044711.png)

使用此正则化项，我们可以防止模型在隐空间中的编码相互远离，并鼓励尽可能多的返回分布发生“重叠”，从而满足预期的连续性和完整性条件。

##### 图自编码器（GAE）

图卷积网络（GCN）可以被视为一种特征提取方法。在处理图结构数据时，GCN通过对节点及其邻居节点的信息进行聚合和转换，生成了新的、更高级的特征表示。

GCN就相当于一个以节点特征和邻接矩阵为输入、以节点嵌入信息为输出的函数，目的是为了得到节点的嵌入向量Z。

GAE是图卷积神经网络GCN在Auto-Encoders (AE)的应用，隐变量Z是图上的N个节点经过GCN后的N*F维特征，编码器就是两层GCN, 解码器就是向量点积。可以将隐变量Z理解为某种意义上图的节点的相似度，通过向量点积得到的两个图节点的相似度越大，则两个节点之间存在边的概率越大。

编码器是简单的两层图卷积网络：
$$
Z=G C N(X, A) 
$$
在这里，X是一个n*×*d的矩阵，其中n是图中节点的数量，d是每个节点的特征数量。A是一个n×n的矩阵，表示图的邻接关系，即节点之间是否存在边。Z是一个n×f的矩阵，其中f是嵌入空间的维度，这个矩阵包含了**每个节点的嵌入表示**。

对于一个两层的图卷积网络（GCN），映射函数会被执行两次。具体来说，每一层的图卷积操作可以被看作是一次映射函数的执行，它将输入的节点特征和图的结构信息（通常表示为邻接矩阵）映射到一个新的节点特征空间。

在第一层，映射函数会接收原始的节点特征和邻接矩阵作为输入，输出的是第一层的节点特征表示。然后，第二层的映射函数会接收第一层的输出作为输入，并再次使用邻接矩阵，产生最终的节点特征表示。这个过程可以看作是两次信息的“聚合”或“传播”，每一次都会考虑节点的邻居信息。

解码器同样是根据两点间存在边的概率来重构图(**节点嵌入表示相似，则两个点之间越有可能相连**）：
$$
\tilde{A}=\operatorname{sigmoid}\left(Z Z^{T}\right) 
$$
损失函数来衡量生成图和原始图间的差异：
$$
L=E_{q(Z | X, A)}[\log p(A | Z)]
$$

##### 变分图自编码器（VGAE）

参考：[VGAE：图变分自编码器 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/345337636)

**Model**

输入为邻接矩阵$A$和节点特征矩阵 $X$, 通过编码器（GCN）可以得到节点向量的低维表示高斯分布$(\mu, \sigma^2)$，然后通过解码器生成图结构（链路预测）。模型架构如下所示：

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/v2-3f8875f537e6aea94ecd2227ef08d84a_720w.webp)

**编码器**

在变分图自编码器（VGAE）中，我们使用变分自动编码器（VAE）代替普通的自动编码器。**在VAE中，编码器不直接输出节点嵌入，而是输出嵌入的均值和方差。**然后，我们可以从这个分布中采样得到节点嵌入。具体来说，编码器可以被表示为：
$$
q(Z | X, A)=\prod_{i=1}^{N} q\left(z_{i} | X, A\right)
$$
这是**节点嵌入的联合分布**。其中
$$
q\left(z_{i} | X, A\right)=N\left(z_{i} | \mu_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right)
$$
$\mu$表示节点向量的均值$\mu=G C N_{\mu}(X, A)$,$\sigma$表示节点向量的方差$\log \sigma=G C N_{\sigma}(X, A)$,这是由两个独立的GCN实现的。

两层图卷积网络的定义如下：
$$
G C N(X, A)=\tilde{A} \operatorname{ReLU}\left(\tilde{A} X W_{0}\right) W_{1}
$$
其中$\tilde{A}=D^{-1 / 2} A D^{-1 / 2}$，是对邻接矩阵进行归一化的结果，其中$D$是一个对角矩阵，其对角线元素是$A$的每一行的和（即每个节点的度）。两个GCN网络的形式都是如此，但是$\mu=G C N_{\mu}(X, A)$和$\log \sigma=G C N_{\sigma}(X, A)$共享$W_{0}$，$W_{1}$不同。采用变量这一步与变分自编码器相同都使用重参数技巧。

**解码器**

和VAE的相同。

#### 



#### Model：Diagram of the proposed graph neural process



在这个框架中，NP的输入是GAE的输出，即每个加法器结构的潜在表示。

##### 神经过程NP

神经过程（Neural Processes，简称NP）是一种新型的深度学习模型，它结合了深度神经网络和高斯过程的优点，以实现数据驱动的、不确定性的预测。

神经过程的主要思想是将数据集视为一个随机过程的观测结果。给定一组输入-输出对（例如，一组点的坐标和对应的函数值），神经过程可以学习这个随机过程，并对新的输入点进行预测。

神经过程的关键特性是它的模型容量可以随数据量的增加而增加。这使得神经过程在处理复杂和多变的数据时具有很大的优势。同时，神经过程还能够量化预测的不确定性，这是许多深度学习模型所不能做到的。

神经过程的一个重要应用是条件生成模型，例如条件图像生成或音频生成。在这些应用中，神经过程可以根据一组条件输入（例如，一组点的坐标）生成一组输出（例如，对应点的像素值或声音样本）。

具体地，NP采用了一个编码器-解码器结构，其中编码器将潜在表示映射到一个低维空间，解码器将低维表示映射回性能指标和不确定性的估计（是不是和VAE很像？）。

在训练过程中，NP使用最大似然估计来优化模型参数，以最大化训练数据的似然性。为了提高模型的泛化能力，NP还使用了一种称为dropout的正则化技术，以减少过拟合的风险。通过这种方式，NP可以学习到加法器结构和性能指标之间的复杂非线性关系，并提供对预测的不确定性的估计。

NP的输出是加法器的性能指标，例如面积、功耗和延迟，以及对这些指标的不确定性的估计。

**NP与GP**

神经过程（Neural Processes，简称NP）与高斯过程（Gaussian Processes，简称GP）有着密切的关系。事实上，神经过程可以被视为高斯过程的一种深度学习扩展。

高斯过程是一种强大的贝叶斯非参数方法，可以在给定数据的情况下对函数进行推断。它的主要优点是能够提供预测的不确定性度量，但是它的计算复杂性随数据量的增加而快速增加，对于大规模数据集来说，这可能是一个问题。

神经过程在某种程度上解决了这个问题。它结合了深度学习的优点，如能够从大规模数据中学习复杂模式的能力，以及高斯过程的优点，如能够提供预测的不确定性度量。神经过程的一个关键特性是它的模型容量可以随数据量的增加而增加，这使得神经过程在处理复杂和多变的数据时具有很大的优势。

总的来说，神经过程可以被视为一种尝试将深度学习和高斯过程的优点结合在一起的方法。

##### 基于顺序优化的DSE框架的工作流程

文中构造的GNP架构如下：

![image-20231029153311973](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231029153311973.png)

其中右下角的NP细节如下：

![图 5.- 神经过程的详细制度。](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/geng5-3114262-small.gif)

在这篇论文中，作者提出了一种基于顺序优化的DSE框架，用于高速加法器的设计空间探索。该框架的工作流程如下： 

* 初始化：首先，选择一组初始加法器结构，并使用EDA工具对其进行物理实现，以获得其性能指标。然后，使用这些数据来训练一个机器学习模型GNP，作为EDA工具的代理模型。 
* 顺序优化：在每次迭代中，利用不确定性信息来指导后续抽样。选择一个候选加法器结构，并使用代理模型GNP预测其性能指标。GNP的输出是加法器的性能指标，例如面积、功耗和延迟，以及对这些指标的不确定性的估计。
* 评估：在每次迭代中，使用EDA工具对所选加法器进行物理实现，并计算其实际性能指标。然后，将这些指标用于更新代理模型的训练数据，以提高其预测精度。在每次迭代中，使用GAE对当前最优加法器结构进行编码，以获得其潜在表示。然后，将这些表示用于训练NP，以预测加法器的性能指标和不确定性的估计。
* 终止条件：重复上述步骤，直到达到预定义的终止条件，例如达到最大迭代次数或找到满足设计目标和约束条件的最优加法器结构。

![image-20231029153237716](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/image-20231029153237716.png)
